1.

    I feel proposal 1 is the better of the two content regulation strategies, since it most upholds the liberty of the end user. Thus, also most reliably allowing for the unbiased truth to be presented. To explain, I must first compare these two proposals. I Initially have to say I like the idea of the fact checking system. It means you always have a second opinion on the authenticity of what you’re reading. Plus, the first party flagging, and third party verification that takes place under the hood, helps to remove bias in that opinion--added emphasis on “helps”. Since, for this system to work as idealized, the parties are entrusted in a lot by the end user. The user must assume the algorithm is fair by not biasing reliable sources to some agenda. As well as assuming the third parties have no agenda of their own, or bias towards that of Facebook’s. In reality this fairness is very much easier said than done. Factors such as, personal bias can unintentionally or intentionally creep in from all people in the process, social pressure to uphold or discredit certain political views can sway decisions, and there is the possibly of corruption getting articles past the system. Thus, while in theory and in good faith, this system of fact checking is helpful, it still faces human limitations. The same human limitations that exist regardless of a fact checking system being in place. Whether or not a news article is trust worthy isn’t being solved but rather contracted. The judgment is given to Facebook’s content regulation, instead of an autonomous decision of the end user. As a result I feel as though trust should neither be fully invested in this system nor fully devoid of the system. It is our own humanity that provides the difficulty in getting to the real facts. Errors, corruption, manipulation, bias; unintentional or intentional these exist in every peg of a fact checking system--and more generally any type of system. This human limitation exists in Facebook users flagging content, the developers making the flagging algorithm, the third party fact checkers, and in yourself. Even if you completely for go a fact checking system, your own judgement still suffers from your humanity. 
    Therefore, this finally brings me to the distinction between these two proposals. Both implement Facebook’s same fact checking system. However, proposal 2 prevents articles deemed problematic being shared altogether, while proposal 1 essentially will just display a warning on that article when it is shared. A fair argument could be made in support for proposal 2, that it is for the end user’s own good to not trust them. Those caught in echo chambers wouldn’t trust Facebook’s claim on the article being problematic. If anything, to them this is further evidence that the article has some merit. Since Facebook is trying to discredit it, they may see it as some higher power, or rich elite group trying to cover up something they don’t want the public to know. In that case its fair to want to censor such a post entirely, as it won’t lead to that article being spread in the first place, nor giving it some false merit. Yet, I feel this only extends the rabbit hole deeper. People seeing Facebook not only disagreeing, but out right censoring their post, can only serve to exasperate their reaction. possibly making people boycott Facebook as a whole and move to platforms of more like minded people and policy. Pushing them further into an epistemic bubble and likely also driving them deeper in an echo chamber. 
    On top of this, I feel that censorship is a slippery slope. If we agree that whatever 3rd-party fact checkers deem as problematic should be censored. That creates a powerful position for these 3rd parties and a greater strength to the question of, what counts as problematic? The answer is obviously dependent, it’s case to case. In other words a grey area. In the case of proposal 2 this subjective grey area is being completely governed by a 3rd party. There is no autonomy by the end user, something can be reported by a user as problematic but that’s the end of it. It’s a one way street, while proposal 1 instead offers both directions--since you can still view what is deemed problematic.
   Instead of jumpng to censorship, I believe we should look to the root of the issue: human nature. I argue that, we limit the power of any one person’s influence over the authenticity of information. Mitigating this issue needs to take place at a personal level, not a social networking one. We each should diversify the opinions we receive about the authenticity of something like an article. In theory, the more opinions, and the more diverse they are, the better. It’s also crucial to try to separate yourself from bias, and look at each opinion as equal. This gives less influence to any one party, and culminates in an outlook on the article that’s an average of all of the individual outlooks. This average is likely to to be closer to the truth than any source alone. As the chances of the same bias, influence, or corruption being present in all opinions is less likely with the more opinions/sources you look to. I find many parallels between this approach and democracy. We place power in the masses in order to limit the power of a few. This philosophy has been key in ensuring peoples liberty in our culture. In the same way, I believe it is key to ensuring the freedom of information.
   It is for this reason that I support proposal 1 over 2. Proposal 2 serves to give a disproportionate amount of power to a single party of fact checkers. this party sits behind the scenes making criticaljudgements for you. On the contrary, proposal 1 doesn’t prioritize any party. It acts to compliment the end user’s own search for truth. the fact checker's judgement serves as second opinon, rather than that of your own. The user is given freedom--all the information is there. Yet with freedom comes responsibility, and it is up to each user to be responsible with the information. Determining the truth is ultimately up to the individual. It requires more effort, but subjectivity is a form of power derived from freedom. As a result, it is better placed in the hands of the masses. Facebook should inform the public: give opinions, offer fact checking, etc. Nudge the user to question the authenticity of the article: to search for the source, hear what different fact checkers have to say, and even fact check for themselves. In doing so, Facebook would not only helping people find the truth but how to find the truth. Nothing should be covered from public view or forced into it. That approach creates more uncertainty than it clears up. We should not legislate the abolishment of fake news, but instead educate the identification of fake news. If we wish to minimize misinformation, we can try to stomp out every article we find. However, like stomping out a weed, it won’t stop a dozen new ones popping up in its place tomorrow. For misinformation to die, we can’t censor or try to beat the truth into the misinformed. We must instead educate and support users towards finding truth. Misinformation won’t need to be hidden if it can’t target an audience. Realistically stopping fake news can’t be done systematically, it must be done personally. It’s a delicate relationship, but truly stopping fake news hinges on breaking people from the echo chambers which feed on it in the first place. 
    
2.

    Proposal 1 best preserves and promotes rights as it does not censor. It preserves the end users right to express whatever they wish, even if it is deemed “problematic” by Facebook. 
	
3.
    
    Proposal 1 best preserves and promotes opportunity of expression as again it doesn’t censor. However, It doesn’t ideally give opportunity of expression. This is because Facebook can peg posts as being “problematic” if their content regulation seems fit. This creates a case where a more powerful entity (Facebook) diminishes the credibility of a person’s post and thus their opportunity to participate in discussion. Nonetheless even with this case. The fact proposal 1 doesn’t censor these posts—while proposal 2 does—means it perseveres and promotes opportunity of expression the most out of the two.
	
4.	
    
    Proposal 1 best preserves and promotes good and equal access to quality and reliable information. This is because like I outlined in question 1, this proposal would not disproportion power to any party. While proposal 2 does censor problematic content, which in theory makes the remaining content more reliable and quality. It diminishes equal access, and places too much power in a single party to determine subjectivity. Due to this single point of failure, it actually is not reliable. Fact checkers are human; making them just as susceptible to human limitations as one’s self. Thus, to extend my thoughts from question 1, the only reliable way to provide access to this type of quality information is to find the common narrative. Essentially done by gathering a diverse number of sources/opinions, and seeing the average of the whole as likely being the truth. This still falls in line with what access to information stands for, as it’s dependent on, “IF we make the effort, we should be able to acquire this information.” This is a crucial statement, as in order for quality information to be reliably available, some effort must be involved. For as I’ve discussed with proposal 2, censoring “problematic” information serves to diminish the reliability of remaining information. It leaves too much subjectivity up to one party. Reliability is sourced from corroboration. For information to be reliable it must be independently verified from multiple sources, the more sources the better. This in essence is what fact checking is all about, finding corroborating evidence for facts and information. However, this is where it gets meta in that fact checking is all about finding multiple sources to verify information, but a fact checker is a single source about the integrity of this information. Thus, even if you’re not fact checking information yourself, you still need to find multiple corroborating fact checkers in order to know the reliability of that information. Hence my point that no matter what, reliability requires some effort to ensure. All of this in mind, proposal 1 comes out on top. It provides the initial information along with a fact checkers opinion on whether or not it’s problematic. Then with some effort to grab further opinions and information, the user can reliably ensure whether or not they have true and quality information. This method does not hinge solely on Facebook’s provided fact checkers but on multiple fact checkers and sources. Therefore, meaning proposal 1 allows for quality and true information to be most reliably given to users.
	
5.	
    
    Proposal 1 best promotes and pursues diverse views and opinions by not censoring any posts. When it comes to proposals 2 censorship method, the content regulation system is very much susceptible to human limitations. Bias, outside influence, or corruption in some part of the system could easily lead to some views being extinguished or artificially promoted. In culmination, leading to less diversity on the platform than that of not censoring “problematic” posts.
	
6.	
    
    Proposal 1 best promotes and pursues communicative power as again it does not involve censorship. Given communicative power means to give people the freedom to challenge the mainstream view, and while the disclaimer that a post may be “problematic” takes away from a poster's equal chance to explore/express radically contrary views. The direct censorship of proposal 2, takes away that chance drastically more so. Thus, proposal 1 does the best job at promoting and pursuing this power.
